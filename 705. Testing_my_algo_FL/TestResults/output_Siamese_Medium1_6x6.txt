[['S' 'F' 'H' 'H' 'F' 'F']
 ['F' 'F' 'F' 'F' 'H' 'F']
 ['F' 'F' 'H' 'H' 'F' 'F']
 ['F' 'F' 'H' 'F' 'F' 'F']
 ['F' 'F' 'F' 'F' 'H' 'F']
 ['F' 'F' 'H' 'F' 'H' 'G']]
Adding thread: now have 13 threads
Starting evaluation
siam score:  0.0042758975
Test reward:  0.0
Q value #end_state_>_threshold:  0
Q value #non_end_state_>threshold:  0
Scores:  {'0.0': 0}
Episodes: 104, frames: 1486, time: 26.387197971343994
training steps:  4
RDN obj mus: [0, 0, 0, 0, 0]
RDN obj sigmas: [1, 1, 1, 1, 1]
104 : 0.0
LR:  3e-06
replay buffer size:  1976
Time Taken :  0.0  mins 26.387046813964844  seconds
[[[350. 217.  62.   3.   0.   0.]
  [211. 159.  46.  18.   3.   0.]
  [ 98.  62.  20.   4.   0.   0.]
  [ 41.  31.   9.   1.   0.   0.]
  [  9.  20.   3.   3.   0.   0.]
  [  1.   7.   3.   1.   0.   0.]]]
deleting a thread, now have 12 threads
Frames:  1833 train batches done:  78 episodes:  118
siam score:  -0.7503923
deleting a thread, now have 11 threads
Frames:  2626 train batches done:  163 episodes:  143
siam score:  -0.7598146
deleting a thread, now have 10 threads
Frames:  3179 train batches done:  251 episodes:  166
siam score:  -0.8109129
deleting a thread, now have 9 threads
Frames:  3737 train batches done:  344 episodes:  187
siam score:  -0.63537455
siam score:  -0.64481616
siam score:  -0.5820149
deleting a thread, now have 8 threads
Frames:  4160 train batches done:  432 episodes:  206
line 193 mcts: sample exp_bonus 0.0
siam score:  -0.5326698
siam score:  -0.56761175
siam score:  -0.5815648
UNIT TEST: sample policy line 217 mcts : [0.2 0.2 0.2 0.2 0.2]
siam score:  -0.5893638
siam score:  -0.6230328
siam score:  -0.55656433
siam score:  -0.5841049
siam score:  -0.61186975
siam score:  -0.67037934
UNIT TEST: sample policy line 217 mcts : [0.083 0.333 0.25  0.25  0.083]
siam score:  -0.7114211
siam score:  -0.75709933
line 193 mcts: sample exp_bonus 0.0
siam score:  -0.780139
siam score:  -0.79241127
siam score:  -0.81406736
siam score:  -0.8100719
siam score:  -0.8109264
Starting evaluation
Test reward:  0.0
Q value #end_state_>_threshold:  0
Q value #non_end_state_>threshold:  0
Scores:  {'0.0': 0}
Episodes: 493, frames: 10401, time: 1004.4227828979492
training steps:  1204
RDN obj mus: [0, 0, 0, 0, 0]
RDN obj sigmas: [1, 1, 1, 1, 1]
493 : 0.0
LR:  0.001
replay buffer size:  11382
Time Taken :  16.0  mins 44.422640800476074  seconds
[[[1730.  806.  203.   12.    0.    0.]
  [1312.  667.  169.   47.    8.    0.]
  [ 941.  407.  109.   12.    0.    0.]
  [ 699.  277.   57.    3.    2.    1.]
  [ 620.  280.   70.   20.    5.    0.]
  [ 982.  381.   76.   11.    1.    0.]]]
line 193 mcts: sample exp_bonus 0.0
siam score:  -0.8500285
siam score:  -0.84535354
siam score:  -0.8557333
siam score:  -0.86624724
siam score:  -0.847818
siam score:  -0.85740614
siam score:  -0.86788404
siam score:  -0.8474233
siam score:  -0.8601205
siam score:  -0.8597293
siam score:  -0.86738324
siam score:  -0.8541188
siam score:  -0.85675216
line 193 mcts: sample exp_bonus 0.0
siam score:  -0.8553428
siam score:  -0.84949756
siam score:  -0.8625888
UNIT TEST: sample policy line 217 mcts : [0.417 0.167 0.167 0.125 0.125]
siam score:  -0.8619668
siam score:  -0.8476667
siam score:  -0.8435737
siam score:  -0.8416719
siam score:  -0.8449145
line 193 mcts: sample exp_bonus 0.0
siam score:  -0.8491947
line 193 mcts: sample exp_bonus 0.0
Starting evaluation
line 193 mcts: sample exp_bonus 0.0
Test reward:  0.0
Q value #end_state_>_threshold:  0
Q value #non_end_state_>threshold:  0
Scores:  {'0.0': 0}
Episodes: 1044, frames: 20226, time: 2320.5961079597473
training steps:  2350
RDN obj mus: [0, 0, 0, 0, 0]
RDN obj sigmas: [1, 1, 1, 1, 1]
1044 : 0.0
LR:  0.001
replay buffer size:  21492
Time Taken :  38.0  mins 40.59596276283264  seconds
[[[3544. 1846.  477.   18.    0.    0.]
  [2642. 1387.  332.   82.   18.    0.]
  [1863.  881.  225.   19.    0.    0.]
  [1282.  600.  136.    4.    2.    1.]
  [ 992.  482.  123.   26.    5.    0.]
  [1437.  612.  133.   12.    1.    0.]]]
siam score:  -0.8537883
siam score:  -0.85328245
siam score:  -0.8588963
line 193 mcts: sample exp_bonus 0.0
siam score:  -0.8475203
siam score:  -0.8424521
siam score:  -0.8585926
siam score:  -0.84958464
siam score:  -0.84851354
siam score:  -0.8522857
siam score:  -0.8525059
Showing what the boot values, the n step returns and the n step returns with V look like
tensor([[    -0.0000],
        [     0.0000],
        [     0.0000],
        [     0.0000],
        [     0.0000],
        [     0.0000],
        [     0.0000],
        [     0.0000],
        [     0.0000],
        [     0.0000]], dtype=torch.float64)
0.0 -1.2162943248968647e-12
0.0 2.4165597143289334e-21
0.0 4.228979500075633e-21
0.0 5.638639333434178e-21
0.0 4.631739452463788e-21
0.0 1.6110398095526221e-21
0.0 4.027599523881555e-21
0.0 2.4165597143289334e-21
0.0 1.6110398095526221e-21
0.0 0.0
siam score:  -0.8521259
siam score:  -0.8444548
UNIT TEST: sample policy line 217 mcts : [0.125 0.292 0.167 0.292 0.125]
siam score:  -0.85600644
line 193 mcts: sample exp_bonus 0.0
line 193 mcts: sample exp_bonus 0.0
siam score:  -0.85054433
siam score:  -0.8502664
siam score:  -0.84593946
siam score:  -0.84907806
siam score:  -0.84128237
Starting evaluation
Test reward:  0.0
Q value #end_state_>_threshold:  0
Q value #non_end_state_>threshold:  0
Scores:  {'0.0': 0}
Episodes: 1591, frames: 30257, time: 3672.9062538146973
training steps:  3538
RDN obj mus: [0, 0, 0, 0, 0]
RDN obj sigmas: [1, 1, 1, 1, 1]
1591 : 0.0
LR:  0.001
replay buffer size:  31988
Time Taken :  61.0  mins 12.906112909317017  seconds
[[[5400. 2846.  714.   30.    0.    0.]
  [4036. 2143.  494.  115.   27.    0.]
  [2807. 1406.  377.   25.    0.    0.]
  [1839.  886.  200.    6.    2.    1.]
  [1432.  738.  172.   35.    6.    0.]
  [1862.  859.  190.   15.    3.    0.]]]
siam score:  -0.85110724
siam score:  -0.8532641
siam score:  -0.84504527
siam score:  -0.8511244
siam score:  -0.85219264
siam score:  -0.85709226
siam score:  -0.8618568
siam score:  -0.8461026
siam score:  -0.8473349
siam score:  -0.85293573
siam score:  -0.84355545
siam score:  -0.84462005
siam score:  -0.854283
siam score:  -0.8458744
Showing what the boot values, the n step returns and the n step returns with V look like
tensor([[    0.0000],
        [    0.0000],
        [    0.0000],
        [    0.0000],
        [    0.0000],
        [    0.0000],
        [    0.0000],
        [   -0.0000],
        [    0.0000],
        [    0.0000]], dtype=torch.float64)
0.0 5.638639333434178e-21
0.0 0.0
0.0 5.638639333434178e-21
0.0 1.1277278666868355e-20
0.0 2.819319666717089e-21
0.0 5.638639333434178e-21
0.0 0.0
0.0 0.0
0.0 1.6110398095526221e-21
0.0 6.4441592382104886e-21
siam score:  -0.8467251
UNIT TEST: sample policy line 217 mcts : [0.167 0.125 0.208 0.375 0.125]
line 193 mcts: sample exp_bonus 0.0
Showing what the boot values, the n step returns and the n step returns with V look like
tensor([[    0.0000],
        [    0.0000],
        [    0.0000],
        [    0.0000],
        [    0.0000],
        [    0.0000],
        [    0.0000],
        [    0.0000],
        [    0.0000],
        [    0.0000]], dtype=torch.float64)
0.0 7.2496791429868e-21
0.0 5.235879381046022e-21
0.0 0.0
0.0 6.041399285822333e-21
0.0 4.027599523881555e-21
0.0 7.2496791429868e-21
0.0 5.235879381046022e-21
0.0 0.0
0.0 5.235879381046022e-21
0.0 0.0
siam score:  -0.8528545
siam score:  -0.84146136
Showing what the boot values, the n step returns and the n step returns with V look like
tensor([[    0.0000],
        [    0.0000],
        [    0.0000],
        [    0.0000],
        [    0.0000],
        [    0.0000],
        [    0.0000],
        [    0.0000],
        [    0.0000],
        [    0.0000]], dtype=torch.float64)
0.0 3.8262195476874775e-21
0.0 3.8262195476874775e-21
0.0 4.833119428657867e-21
0.0 4.833119428657867e-21
0.0 0.0
0.0 4.833119428657867e-21
0.0 6.041399285822333e-21
0.0 0.0
0.0 4.4303594762697106e-21
0.0 2.819319666717089e-21
siam score:  -0.8468089
siam score:  -0.8441505
siam score:  -0.85525924
Starting evaluation
Test reward:  0.0
Q value #end_state_>_threshold:  0
Q value #non_end_state_>threshold:  0
Scores:  {'0.0': 0}
Episodes: 2105, frames: 40294, time: 4713.3885860443115
training steps:  4723
RDN obj mus: [0, 0, 0, 0, 0]
RDN obj sigmas: [1, 1, 1, 1, 1]
2105 : 0.0
LR:  0.001
replay buffer size:  42460
Time Taken :  78.0  mins 33.388449907302856  seconds
[[[7410. 3951.  957.   39.    0.    0.]
  [5462. 2899.  650.  155.   37.    0.]
  [3721. 1897.  500.   38.    0.    0.]
  [2411. 1193.  266.    9.    3.    1.]
  [1767.  969.  246.   55.    8.    0.]
  [2216. 1066.  229.   28.    6.    0.]]]
siam score:  -0.8455647
siam score:  -0.85505104
siam score:  -0.8529041
line 193 mcts: sample exp_bonus 0.0
siam score:  -0.84571725
siam score:  -0.84586424
siam score:  -0.84273374
siam score:  -0.84997666
siam score:  -0.8453724
siam score:  -0.84684354
siam score:  -0.84771925
line 193 mcts: sample exp_bonus 0.0
line 193 mcts: sample exp_bonus 0.0
siam score:  -0.8535808
siam score:  -0.8553746
siam score:  -0.85778314
siam score:  -0.85052127
siam score:  -0.85108465
siam score:  -0.85130334
siam score:  -0.844247
siam score:  -0.8515293
siam score:  -0.84097886
siam score:  -0.8438536
line 193 mcts: sample exp_bonus 0.0
siam score:  -0.85277313
Starting evaluation
Test reward:  0.0
Q value #end_state_>_threshold:  0
Q value #non_end_state_>threshold:  0
Scores:  {'0.0': 0}
Episodes: 2628, frames: 50160, time: 5717.182194948196
training steps:  5869
RDN obj mus: [0, 0, 0, 0, 0]
RDN obj sigmas: [1, 1, 1, 1, 1]
2628 : 0.0
LR:  0.001
replay buffer size:  52785
Time Taken :  95.0  mins 17.182049989700317  seconds
[[[9413. 5043. 1196.   45.    0.    0.]
  [6834. 3669.  861.  196.   51.    0.]
  [4603. 2376.  634.   46.    0.    0.]
  [2915. 1476.  327.   11.    3.    1.]
  [2099. 1204.  294.   76.   13.    0.]
  [2578. 1247.  280.   34.    7.    0.]]]
siam score:  -0.83558637
siam score:  -0.8373901
siam score:  -0.8441574
siam score:  -0.8400249
siam score:  -0.83926773
siam score:  -0.83139193
siam score:  -0.8302361
siam score:  -0.833065
siam score:  -0.834071
siam score:  -0.82495666
siam score:  -0.8197989
siam score:  -0.81918794
UNIT TEST: sample policy line 217 mcts : [0.125 0.375 0.25  0.125 0.125]
siam score:  -0.8346323
UNIT TEST: sample policy line 217 mcts : [0.292 0.208 0.167 0.208 0.125]
siam score:  -0.82959765
siam score:  -0.82590616
siam score:  -0.8249973
siam score:  -0.8177484
siam score:  -0.8201973
line 193 mcts: sample exp_bonus 0.0
siam score:  -0.8233834
Showing what the boot values, the n step returns and the n step returns with V look like
tensor([[     0.0000],
        [     0.0000],
        [     0.0000],
        [    -0.0000],
        [     0.0000],
        [    -0.0000],
        [     0.0000],
        [     0.0000],
        [     0.0000],
        [     0.0000]], dtype=torch.float64)
0.0 1.7298408288815245e-12
0.0 0.0
0.0 0.0
0.0 0.0
0.0 1.3838726650384674e-12
0.0 -6.81124822377225e-13
0.0 3.8921418709241393e-13
0.0 9.514124579187762e-13
0.0 9.514124587242962e-13
0.0 0.0
line 193 mcts: sample exp_bonus 0.0
siam score:  -0.83149916
siam score:  -0.8276161
siam score:  -0.82438433
siam score:  -0.8219063
siam score:  -0.82884806
siam score:  -0.82008314
siam score:  -0.8215503
siam score:  -0.8299193
siam score:  -0.82870275
UNIT TEST: sample policy line 217 mcts : [0.167 0.167 0.167 0.375 0.125]
UNIT TEST: sample policy line 217 mcts : [0.167 0.125 0.167 0.417 0.125]
siam score:  -0.825233
siam score:  -0.8244255
Starting evaluation
Test reward:  0.0
Q value #end_state_>_threshold:  0
Q value #non_end_state_>threshold:  0
Scores:  {'0.0': 0}
Episodes: 3184, frames: 60352, time: 6762.792207956314
training steps:  7074
RDN obj mus: [0, 0, 0, 0, 0]
RDN obj sigmas: [1, 1, 1, 1, 1]
3184 : 0.0
LR:  0.001
replay buffer size:  63377
Time Taken :  112.0  mins 42.7920618057251  seconds
[[[11461.  6172.  1458.    54.     0.     0.]
  [ 8394.  4471.  1066.   247.    66.     0.]
  [ 5427.  2806.   764.    59.     0.     0.]
  [ 3379.  1761.   396.    14.     3.     1.]
  [ 2458.  1442.   357.    90.    14.     0.]
  [ 2974.  1455.   334.    37.     8.     0.]]]
siam score:  -0.8207454
UNIT TEST: sample policy line 217 mcts : [0.167 0.167 0.167 0.167 0.333]
siam score:  -0.8206153
siam score:  -0.8192784
UNIT TEST: sample policy line 217 mcts : [0.125 0.125 0.458 0.167 0.125]
line 193 mcts: sample exp_bonus 0.0
line 193 mcts: sample exp_bonus 0.0
siam score:  -0.8300161
siam score:  -0.8287868
siam score:  -0.82700676
siam score:  -0.8298423
Showing what the boot values, the n step returns and the n step returns with V look like
tensor([[    0.0000],
        [    0.0000],
        [    0.0000],
        [    0.0000],
        [    0.0000],
        [    0.0000],
        [    0.0000],
        [    0.0000],
        [    0.0000],
        [    0.0000]], dtype=torch.float64)
0.0 0.0
0.0 0.0
0.0 6.751784967530676e-12
0.0 8.768130678639808e-12
0.0 0.0
0.0 8.178903647159161e-12
0.0 5.373318062597713e-12
0.0 0.0
0.0 7.00044958601029e-12
0.0 0.0
UNIT TEST: sample policy line 217 mcts : [0.167 0.333 0.25  0.125 0.125]
siam score:  -0.8205499
line 193 mcts: sample exp_bonus 0.0
siam score:  -0.82404405
siam score:  -0.82407916
siam score:  -0.8258893
siam score:  -0.82692456
siam score:  -0.83470607
Showing what the boot values, the n step returns and the n step returns with V look like
tensor([[     0.0000],
        [    -0.0000],
        [    -0.0000],
        [    -0.0000],
        [     0.0000],
        [     0.0000],
        [     0.0000],
        [    -0.0000],
        [    -0.0000],
        [     0.0000]], dtype=torch.float64)
0.0 5.481433118707305e-12
0.0 -2.2487930659867714e-12
0.0 -9.838469621654054e-13
0.0 -5.621982611601235e-13
0.0 2.3893426430575308e-12
0.0 1.9676939424550085e-12
0.0 0.0
0.0 -8.108628816535367e-13
0.0 -2.2487930659867714e-12
0.0 0.0
siam score:  -0.82844895
line 193 mcts: sample exp_bonus 0.0
siam score:  -0.8227837
siam score:  -0.80947566
siam score:  -0.8108619
siam score:  -0.811824
siam score:  -0.82583845
siam score:  -0.8290233
siam score:  -0.8269588
siam score:  -0.8287662
Showing what the boot values, the n step returns and the n step returns with V look like
tensor([[    -0.0000],
        [     0.0000],
        [    -0.0000],
        [    -0.0000],
        [    -0.0000],
        [     0.0000],
        [    -0.0000],
        [     0.0000],
        [     0.0000],
        [    -0.0000]], dtype=torch.float64)
0.0 -2.4974576832581047e-12
0.0 1.4919877169190802e-12
0.0 -2.7677453099389373e-12
0.0 -1.4811761983191124e-12
0.0 0.0
0.0 1.3081921306955621e-12
0.0 0.0
0.0 0.0
0.0 1.1352080477671338e-12
0.0 0.0
siam score:  -0.8245542
siam score:  -0.83320457
siam score:  -0.83192044
Starting evaluation
Test reward:  0.0
Q value #end_state_>_threshold:  0
Q value #non_end_state_>threshold:  0
Scores:  {'0.0': 0}
Episodes: 3711, frames: 70233, time: 7764.564443826675
training steps:  8220
RDN obj mus: [0, 0, 0, 0, 0]
RDN obj sigmas: [1, 1, 1, 1, 1]
3711 : 0.0
LR:  0.001
replay buffer size:  73720
Time Taken :  129.0  mins 24.564301013946533  seconds
[[[13327.  7327.  1706.    62.     0.     0.]
  [ 9687.  5268.  1247.   295.    77.     0.]
  [ 6314.  3315.   903.    71.     0.     0.]
  [ 3930.  2083.   453.    16.     3.     1.]
  [ 2861.  1635.   386.    99.    15.     0.]
  [ 3359.  1651.   383.    40.     8.     0.]]]
UNIT TEST: sample policy line 217 mcts : [0.167 0.125 0.167 0.25  0.292]
siam score:  -0.8207608
siam score:  -0.82038224
UNIT TEST: sample policy line 217 mcts : [0.25  0.333 0.167 0.125 0.125]
siam score:  -0.82629853
siam score:  -0.8337691
siam score:  -0.82760286
siam score:  -0.82462
siam score:  -0.82619566
Showing what the boot values, the n step returns and the n step returns with V look like
tensor([[    -0.0000],
        [    -0.0000],
        [    -0.0000],
        [    -0.0000],
        [    -0.0000],
        [    -0.0000],
        [    -0.0000],
        [    -0.0000],
        [    -0.0000],
        [    -0.0000]], dtype=torch.float64)
0.0 -2.2541988204536356e-12
0.0 0.0
0.0 0.0
0.0 -3.567796691519464e-12
0.0 -8.973549253329306e-13
0.0 -5.254391492318512e-12
0.0 -4.054314425183601e-12
0.0 -3.443464385703117e-12
0.0 -5.254391492318512e-12
0.0 -1.5352337280856384e-12
line 193 mcts: sample exp_bonus 0.0
siam score:  -0.82899386
siam score:  -0.8339874
siam score:  -0.82226694
siam score:  -0.8264875
siam score:  -0.83086944
siam score:  -0.8279103
line 193 mcts: sample exp_bonus 0.0
siam score:  -0.8182352
Showing what the boot values, the n step returns and the n step returns with V look like
tensor([[    -0.0000],
        [    -0.0000],
        [    -0.0000],
        [    -0.0000],
        [    -0.0000],
        [    -0.0000],
        [    -0.0000],
        [    -0.0000],
        [    -0.0000],
        [    -0.0000]], dtype=torch.float64)
0.0 -8.216743902650575e-12
0.0 0.0
0.0 -1.4271186661360268e-12
0.0 -1.2541345900545176e-12
0.0 -1.4271186661360268e-12
0.0 -3.2650745412595247e-12
0.0 0.0
0.0 -8.281612936114951e-12
0.0 -1.2973806100817947e-12
0.0 0.0
siam score:  -0.8337778
siam score:  -0.8345099
line 193 mcts: sample exp_bonus 0.0
siam score:  -0.8375685
siam score:  -0.8267523
siam score:  -0.8355855
siam score:  -0.83939517
siam score:  -0.8246621
siam score:  -0.8288852
line 193 mcts: sample exp_bonus 0.0
siam score:  -0.82071394
line 193 mcts: sample exp_bonus 0.0
siam score:  -0.8276215
siam score:  -0.8272455
Starting evaluation
Test reward:  0.0
Q value #end_state_>_threshold:  0
Q value #non_end_state_>threshold:  0
Scores:  {'0.0': 0}
Episodes: 4252, frames: 80199, time: 9252.108593940735
training steps:  9332
RDN obj mus: [0, 0, 0, 0, 0]
RDN obj sigmas: [1, 1, 1, 1, 1]
4252 : 0.0
LR:  0.001
replay buffer size:  84020
Time Taken :  154.0  mins 12.108452081680298  seconds
[[[15391.  8450.  1960.    70.     0.     0.]
  [11113.  5999.  1425.   337.    86.     0.]
  [ 7134.  3797.  1041.    83.     0.     0.]
  [ 4461.  2421.   528.    28.     3.     1.]
  [ 3193.  1842.   463.   126.    20.     0.]
  [ 3679.  1816.   420.    52.     8.     0.]]]
line 193 mcts: sample exp_bonus 0.0
siam score:  -0.83276194
siam score:  -0.81357527
siam score:  -0.8286353
line 193 mcts: sample exp_bonus 0.0
siam score:  -0.8255579
siam score:  -0.8277583
siam score:  -0.8367375
siam score:  -0.8252768
UNIT TEST: sample policy line 217 mcts : [0.167 0.125 0.167 0.292 0.25 ]
siam score:  -0.8323629
siam score:  -0.82049894
siam score:  -0.8221595
siam score:  -0.821304
line 193 mcts: sample exp_bonus 0.0
siam score:  -0.82777125
siam score:  -0.82044023
line 193 mcts: sample exp_bonus 0.0
siam score:  -0.81043345
Starting evaluation
Test reward:  0.0
Q value #end_state_>_threshold:  0
Q value #non_end_state_>threshold:  0
Scores:  {'0.0': 0}
Episodes: 4828, frames: 90125, time: 11760.273736000061
training steps:  10345
RDN obj mus: [0, 0, 0, 0, 0]
RDN obj sigmas: [1, 1, 1, 1, 1]
4828 : 0.0
LR:  0.001
replay buffer size:  94302
Time Taken :  196.0  mins 0.2735910415649414  seconds
[[[17612.  9602.  2236.    78.     0.     0.]
  [12507.  6792.  1620.   384.    97.     0.]
  [ 7900.  4272.  1196.    95.     1.     1.]
  [ 4897.  2681.   596.    37.     6.     2.]
  [ 3511.  2032.   504.   145.    24.     0.]
  [ 3986.  1949.   459.    64.    11.     0.]]]
siam score:  -0.8204733
siam score:  -0.82003754
siam score:  -0.8202435
siam score:  -0.8237923
Showing what the boot values, the n step returns and the n step returns with V look like
tensor([[    0.0000],
        [    0.0000],
        [    0.0000],
        [    0.0000],
        [    0.0000],
        [    0.0000],
        [    0.0000],
        [    0.0000],
        [    0.0000],
        [    0.0000]], dtype=torch.float64)
0.0 3.399137219091998e-11
0.0 0.0
0.0 0.0
0.0 3.3612969509136455e-11
0.0 2.9526220561179267e-11
0.0 3.669424847625526e-11
0.0 0.0
0.0 0.0
0.0 4.2997355978702916e-11
0.0 4.579753581141544e-11
siam score:  -0.8228711
siam score:  -0.82308507
siam score:  -0.82462454
siam score:  -0.8108
siam score:  -0.8138239
UNIT TEST: sample policy line 217 mcts : [0.125 0.292 0.208 0.208 0.167]
UNIT TEST: sample policy line 217 mcts : [0.25  0.125 0.125 0.292 0.208]
siam score:  -0.82572645
siam score:  -0.82411295
line 193 mcts: sample exp_bonus 0.0
siam score:  -0.82626206
siam score:  -0.8203439
siam score:  -0.83084345
siam score:  -0.82155836
siam score:  -0.8141516
UNIT TEST: sample policy line 217 mcts : [0.25  0.125 0.167 0.125 0.333]
siam score:  -0.8229585
siam score:  -0.8221485
siam score:  -0.82724327
line 193 mcts: sample exp_bonus 0.0
siam score:  -0.8302458
siam score:  -0.8157371
siam score:  -0.8111902
siam score:  -0.820124
siam score:  -0.82324004
siam score:  -0.817959
Starting evaluation
Test reward:  0.0
Q value #end_state_>_threshold:  0
Q value #non_end_state_>threshold:  0
Scores:  {'0.0': 0}
Episodes: 5378, frames: 100148, time: 14209.546032905579
training steps:  11342
RDN obj mus: [0, 0, 0, 0, 0]
RDN obj sigmas: [1, 1, 1, 1, 1]
5378 : 0.0
LR:  0.001
replay buffer size:  104701
Time Taken :  236.0  mins 49.545896768569946  seconds
[[[19525. 10620.  2486.    83.     0.     0.]
  [13919.  7585.  1819.   410.   106.     0.]
  [ 8770.  4790.  1339.   103.     1.     1.]
  [ 5520.  3017.   668.    46.     8.     2.]
  [ 3916.  2263.   577.   164.    28.     0.]
  [ 4285.  2120.   517.    71.    11.     0.]]]
siam score:  -0.826864
siam score:  -0.82353014
siam score:  -0.82348055
siam score:  -0.82077295
siam score:  -0.81751895
siam score:  -0.81374407
line 193 mcts: sample exp_bonus 0.0
siam score:  -0.824126
UNIT TEST: sample policy line 217 mcts : [0.25  0.125 0.333 0.125 0.167]
line 193 mcts: sample exp_bonus 0.0
siam score:  -0.82596403
siam score:  -0.82601595
siam score:  -0.81448275
siam score:  -0.81765157
line 193 mcts: sample exp_bonus 0.0
line 193 mcts: sample exp_bonus 0.0
line 193 mcts: sample exp_bonus 0.0
siam score:  -0.8173322
siam score:  -0.8178119
siam score:  -0.8124424
line 193 mcts: sample exp_bonus 0.0
siam score:  -0.8271375
UNIT TEST: sample policy line 217 mcts : [0.417 0.125 0.167 0.167 0.125]
UNIT TEST: sample policy line 217 mcts : [0.125 0.458 0.167 0.125 0.125]
siam score:  -0.8257308
siam score:  -0.8162024
siam score:  -0.8197008
siam score:  -0.81407034
siam score:  -0.81298727
siam score:  -0.8188672
Starting evaluation
siam score:  -0.8175891
Test reward:  0.0
Q value #end_state_>_threshold:  0
Q value #non_end_state_>threshold:  0
Scores:  {'0.0': 0}
Episodes: 5921, frames: 110156, time: 16585.960757017136
training steps:  12435
RDN obj mus: [0, 0, 0, 0, 0]
RDN obj sigmas: [1, 1, 1, 1, 1]
5921 : 0.0
LR:  0.001
replay buffer size:  115221
Time Taken :  276.0  mins 25.960612773895264  seconds
[[[21448. 11706.  2734.    87.     0.     0.]
  [15306.  8375.  2020.   453.   111.     0.]
  [ 9673.  5341.  1484.   114.     1.     1.]
  [ 6063.  3378.   740.    50.     8.     2.]
  [ 4290.  2501.   623.   177.    31.     0.]
  [ 4586.  2279.   566.    76.    11.     0.]]]
siam score:  -0.82396394
siam score:  -0.8287358
UNIT TEST: sample policy line 217 mcts : [0.417 0.125 0.167 0.125 0.167]
siam score:  -0.8304455
siam score:  -0.81273806
line 193 mcts: sample exp_bonus 0.0
siam score:  -0.82570297
siam score:  -0.81226575
siam score:  -0.8227104
siam score:  -0.8192465
siam score:  -0.80908155
siam score:  -0.8204141
siam score:  -0.82105374
siam score:  -0.82058144
siam score:  -0.8142352
siam score:  -0.81661326
siam score:  -0.82014185
siam score:  -0.82136154
line 193 mcts: sample exp_bonus 0.0
siam score:  -0.8315444
siam score:  -0.82600754
siam score:  -0.8182163
siam score:  -0.8247531
siam score:  -0.82769305
siam score:  -0.82826483
siam score:  -0.8262108
siam score:  -0.81500185
siam score:  -0.8162761
siam score:  -0.81856465
siam score:  -0.8280414
siam score:  -0.82680696
line 193 mcts: sample exp_bonus 0.0
siam score:  -0.81956345
UNIT TEST: sample policy line 217 mcts : [0.125 0.125 0.25  0.375 0.125]
UNIT TEST: sample policy line 217 mcts : [0.167 0.208 0.167 0.292 0.167]
siam score:  -0.82856995
siam score:  -0.8299937
siam score:  -0.8283517
siam score:  -0.8156749
Starting evaluation
Test reward:  0.0
Q value #end_state_>_threshold:  0
Q value #non_end_state_>threshold:  0
Scores:  {'0.0': 0}
Episodes: 6483, frames: 120209, time: 18778.825019836426
training steps:  14076
RDN obj mus: [0, 0, 0, 0, 0]
RDN obj sigmas: [1, 1, 1, 1, 1]
6483 : 0.0
LR:  0.001
replay buffer size:  125731
Time Taken :  312.0  mins 58.8248770236969  seconds
[[[23367. 12749.  2989.   100.     0.     0.]
  [16714.  9168.  2234.   495.   118.     0.]
  [10508.  5847.  1635.   125.     4.     4.]
  [ 6657.  3721.   802.    56.    10.     2.]
  [ 4719.  2751.   700.   201.    37.     0.]
  [ 4853.  2447.   619.    82.    12.     0.]]]
siam score:  -0.8263596
siam score:  -0.8238858
line 193 mcts: sample exp_bonus 0.0
siam score:  -0.8202471
siam score:  -0.8184834
siam score:  -0.8300005
line 193 mcts: sample exp_bonus 0.0
siam score:  -0.80871123
line 193 mcts: sample exp_bonus 0.0
UNIT TEST: sample policy line 217 mcts : [0.333 0.125 0.167 0.208 0.167]
siam score:  -0.8241814
Showing what the boot values, the n step returns and the n step returns with V look like
tensor([[    -0.0000],
        [    -0.0000],
        [    -0.0000],
        [    -0.0000],
        [    -0.0000],
        [    -0.0000],
        [    -0.0000],
        [    -0.0000],
        [    -0.0000],
        [    -0.0000]], dtype=torch.float64)
0.0 -8.319453210495807e-11
0.0 -8.310804006450076e-11
0.0 -8.562712076240424e-11
0.0 -8.319453210495807e-11
0.0 0.0
0.0 -8.748669964814551e-11
0.0 -9.868741897577353e-11
0.0 -9.005983787138516e-11
0.0 0.0
0.0 -9.293569823763507e-11
Sims:  25 1 epoch:  125984 pick best:  False frame count:  125984
siam score:  -0.8170436
line 193 mcts: sample exp_bonus 0.0
siam score:  -0.8201709
siam score:  -0.82629526
siam score:  -0.8214297
siam score:  -0.81902254
siam score:  -0.81309867
siam score:  -0.8150427
siam score:  -0.81451225
UNIT TEST: sample policy line 217 mcts : [0.125 0.208 0.25  0.292 0.125]
siam score:  -0.8153412
siam score:  -0.81524795
Starting evaluation
Test reward:  0.0
Q value #end_state_>_threshold:  0
Q value #non_end_state_>threshold:  0
Scores:  {'0.0': 0}
Episodes: 7015, frames: 130202, time: 20448.65981912613
training steps:  15245
RDN obj mus: [0, 0, 0, 0, 0]
RDN obj sigmas: [1, 1, 1, 1, 1]
7015 : 0.0
LR:  0.001
replay buffer size:  136139
Time Taken :  340.0  mins 48.659674882888794  seconds
[[[25455. 13885.  3234.   111.     0.     0.]
  [18110.  9987.  2415.   531.   125.     1.]
  [11316.  6350.  1775.   133.     7.     9.]
  [ 7126.  4037.   867.    61.    15.     6.]
  [ 5047.  2954.   756.   217.    39.     1.]
  [ 5193.  2653.   670.    88.    13.     0.]]]
UNIT TEST: sample policy line 217 mcts : [0.125 0.125 0.167 0.125 0.458]
siam score:  -0.8301847
siam score:  -0.8293278
siam score:  -0.82881516
UNIT TEST: sample policy line 217 mcts : [0.125 0.167 0.167 0.125 0.417]
siam score:  -0.82133347
siam score:  -0.8148592
siam score:  -0.8171893
siam score:  -0.8211814
siam score:  -0.8221739
siam score:  -0.8220559
siam score:  -0.8226092
siam score:  -0.81448317
siam score:  -0.8252222
line 193 mcts: sample exp_bonus 0.0
line 193 mcts: sample exp_bonus 0.0
siam score:  -0.8289111
siam score:  -0.82487345
siam score:  -0.8248262
step number:  64 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
siam score:  -0.8062804
UNIT TEST: sample policy line 217 mcts : [0.125 0.125 0.167 0.167 0.417]
siam score:  -0.78490585
siam score:  -0.7949681
siam score:  -0.79576665
line 193 mcts: sample exp_bonus 0.0
siam score:  -0.7950425
line 193 mcts: sample exp_bonus 0.0
UNIT TEST: sample policy line 217 mcts : [0.042 0.042 0.75  0.125 0.042]
siam score:  -0.81972456
line 193 mcts: sample exp_bonus 0.0
siam score:  -0.8297233
line 193 mcts: sample exp_bonus 0.0
Starting evaluation
siam score:  -0.8174305
Test reward:  0.0
Q value #end_state_>_threshold:  0
Q value #non_end_state_>threshold:  0
Scores:  {'0.0': 1.0}
Episodes: 7415, frames: 140488, time: 22003.167031764984
training steps:  16223
RDN obj mus: [0, 0, 0, 0, 0]
RDN obj sigmas: [1, 1, 1, 1, 1]
7415 : 0.008333333333333333
LR:  0.001
replay buffer size:  148425
Time Taken :  366.0  mins 43.16688895225525  seconds
[[[27339. 14701.  3392.   117.     0.     0.]
  [19426. 10572.  2539.   568.   132.     4.]
  [12143.  6718.  1846.   145.    13.    17.]
  [ 7987.  4359.   923.    68.    20.    21.]
  [ 5883.  3232.   833.   243.    42.     5.]
  [ 5923.  3020.   722.   102.    17.     1.]]]
siam score:  -0.81668603
siam score:  -0.8067372
siam score:  -0.80837846
siam score:  -0.81656617
UNIT TEST: sample policy line 217 mcts : [0.25  0.125 0.25  0.167 0.208]
siam score:  -0.821043
siam score:  -0.8373574
siam score:  -0.8180414
siam score:  -0.8175401
siam score:  -0.82156354
UNIT TEST: sample policy line 217 mcts : [0.125 0.125 0.167 0.458 0.125]
siam score:  -0.82555026
siam score:  -0.8216102
line 193 mcts: sample exp_bonus 0.0
siam score:  -0.83842397
Showing what the boot values, the n step returns and the n step returns with V look like
tensor([[0.0010],
        [0.0011],
        [0.0000],
        [0.0012],
        [0.0004],
        [0.0000],
        [0.0011],
        [0.0009],
        [0.0010],
        [0.0009]], dtype=torch.float64)
0.0 0.000952703484934033
0.0 0.001112521453246526
0.0 0.0
0.0 0.0011695784626022095
0.0 0.0004239170451007988
0.0 0.0
0.0 0.001112521453246526
0.0 0.0009125843221939366
0.0 0.0010211247694240071
0.0 0.0009125843221939366
line 193 mcts: sample exp_bonus 0.0
siam score:  -0.8635951
line 193 mcts: sample exp_bonus 0.0
siam score:  -0.8629187
siam score:  -0.8598611
siam score:  -0.8623166
Starting evaluation
siam score:  -0.8610144
step number:  58 total reward:  1.0  reward:  1.0 rdn_beta:  0
Test reward:  0.04
Q value #end_state_>_threshold:  0
Q value #non_end_state_>threshold:  1
Scores:  {'0.0': 1.0}
Episodes: 7661, frames: 150381, time: 23258.87834405899
training steps:  17005
RDN obj mus: [0, 0, 0, 0, 0]
RDN obj sigmas: [1, 1, 1, 1, 1]
7661 : 0.0
LR:  0.001
replay buffer size:  160121
Time Taken :  387.0  mins 38.8782000541687  seconds
[[[29408. 15280.  3452.   121.     0.     0.]
  [21014. 11093.  2594.   581.   137.     4.]
  [13217.  7041.  1888.   153.    20.    18.]
  [ 8845.  4598.   951.    85.    33.    29.]
  [ 6634.  3505.   914.   307.    54.     7.]
  [ 6570.  3238.   760.   148.    20.     1.]]]
siam score:  -0.8565707
UNIT TEST: sample policy line 217 mcts : [0.25  0.042 0.292 0.25  0.167]
siam score:  -0.8541083
step number:  40 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
line 193 mcts: sample exp_bonus 0.0
step number:  47 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
siam score:  -0.8394972
step number:  38 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
siam score:  -0.8392027
line 193 mcts: sample exp_bonus 0.0
siam score:  -0.87492263
siam score:  -0.8727343
UNIT TEST: sample policy line 217 mcts : [0.167 0.125 0.417 0.167 0.125]
step number:  36 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
line 193 mcts: sample exp_bonus 0.0
step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
step number:  47 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
siam score:  -0.78776914
siam score:  -0.792574
step number:  29 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
siam score:  -0.80774164
step number:  31 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
siam score:  -0.7773027
siam score:  -0.77547854
step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
step number:  61 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
step number:  41 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
step number:  76 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
step number:  47 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
step number:  36 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
siam score:  -0.80247307
step number:  41 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
step number:  37 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
siam score:  -0.8175632
step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
siam score:  -0.812519
step number:  49 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
step number:  50 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
step number:  38 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
siam score:  -0.7948613
step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
siam score:  -0.81399095
step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
step number:  34 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
step number:  35 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Starting evaluation
step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0
step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0
step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0
step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0
step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0
step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0
step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0
step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0
step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0
step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0
step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0
step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0
step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0
step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0
step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0
step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0
step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0
step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  0
step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Test reward:  0.72
Q value #end_state_>_threshold:  60
Q value #non_end_state_>threshold:  88
Scores:  {'0.0': 34.0}
Episodes: 7958, frames: 160097, time: 24603.29447698593
training steps:  18064
RDN obj mus: [0, 0, 0, 0, 0]
RDN obj sigmas: [1, 1, 1, 1, 1]
7958 : 0.2
LR:  0.001
replay buffer size:  170210
Time Taken :  410.0  mins 3.2943358421325684  seconds
[[[30687. 15564.  3484.   123.     0.     0.]
  [22270. 11417.  2622.   588.   138.     5.]
  [14291.  7355.  1928.   166.    40.    42.]
  [ 9884.  4996.  1022.   191.   132.   160.]
  [ 7521.  3947.  1099.   428.    74.    90.]
  [ 7301.  3523.   829.   166.    22.    34.]]]
step number:  29 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
siam score:  -0.8483824
step number:  32 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
step number:  38 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
siam score:  -0.82626706
UNIT TEST: sample policy line 217 mcts : [0.042 0.042 0.792 0.042 0.083]
UNIT TEST: sample policy line 217 mcts : [0.125 0.083 0.25  0.417 0.125]
siam score:  -0.8209283
step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
siam score:  -0.8247861
siam score:  -0.8260882
step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
step number:  45 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
siam score:  -0.82794845
siam score:  -0.8231739
step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
line 193 mcts: sample exp_bonus 0.0
step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
step number:  28 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
siam score:  -0.867997
step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
step number:  41 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
siam score:  -0.8472383
step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
line 193 mcts: sample exp_bonus 0.0
step number:  48 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
step number:  30 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
step number:  56 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
step number:  33 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
step number:  49 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
step number:  42 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
step number:  36 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
step number:  28 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
siam score:  -0.81302404
UNIT TEST: sample policy line 217 mcts : [0.292 0.125 0.167 0.25  0.167]
siam score:  -0.8260127
step number:  42 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
siam score:  -0.83737546
step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
step number:  33 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
line 193 mcts: sample exp_bonus 0.0
step number:  50 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
step number:  31 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
siam score:  -0.85991704
siam score:  -0.85954934
step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
step number:  30 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
step number:  33 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
step number:  31 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
step number:  66 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
step number:  65 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
step number:  39 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
siam score:  -0.8645973
step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
step number:  34 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
step number:  32 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
siam score:  -0.8397995
step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
step number:  37 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
siam score:  -0.84083086
step number:  53 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
step number:  47 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
siam score:  -0.83277816
step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
siam score:  -0.8273501
siam score:  -0.8297301
step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
step number:  33 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
step number:  28 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
siam score:  -0.8281419
siam score:  -0.8266567
step number:  31 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
step number:  39 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
siam score:  -0.82600296
step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
step number:  30 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
step number:  36 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
siam score:  -0.82458586
siam score:  -0.8228169
step number:  43 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
step number:  33 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
step number:  29 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
line 193 mcts: sample exp_bonus 0.0
siam score:  -0.83967227
step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
step number:  42 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
siam score:  -0.8370749
step number:  32 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
siam score:  -0.84637237
siam score:  -0.84507614
step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
step number:  34 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
step number:  27 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
step number:  33 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
step number:  28 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
siam score:  -0.89328
step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
siam score:  -0.896873
step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
siam score:  -0.90289325
step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
siam score:  -0.905432
step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
step number:  28 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
siam score:  -0.8956552
step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
siam score:  -0.8960618
step number:  35 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
step number:  57 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
step number:  31 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
step number:  42 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
step number:  40 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
step number:  40 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
siam score:  -0.882431
step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
step number:  33 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
step number:  24 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
step number:  29 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
line 193 mcts: sample exp_bonus 0.0
siam score:  -0.8937534
step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
step number:  30 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
step number:  16 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
step number:  28 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
siam score:  -0.8999188
step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
step number:  31 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
step number:  20 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
step number:  31 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
step number:  25 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
step number:  34 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
step number:  33 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
step number:  37 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
siam score:  -0.8915152
step number:  22 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
step number:  23 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
step number:  17 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
Starting evaluation
step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
step number:  21 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0
step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0
step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0
step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0
step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0
step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0
step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0
step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0
step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0
step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0
step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0
step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0
step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0
step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0
step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0
step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0
step number:  13 total reward:  1.0  reward:  1.0 rdn_beta:  0
siam score:  -0.8846286
step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0
step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0
step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0
step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0
step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0
step number:  14 total reward:  1.0  reward:  1.0 rdn_beta:  0
step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0
step number:  15 total reward:  1.0  reward:  1.0 rdn_beta:  0
Test reward:  1.0
Q value #end_state_>_threshold:  465
Q value #non_end_state_>threshold:  1524
Scores:  {'0.0': 255.0}
Episodes: 8448, frames: 170198, time: 26504.439247846603
training steps:  19939
RDN obj mus: [0, 0, 0, 0, 0]
RDN obj sigmas: [1, 1, 1, 1, 1]
8448 : 0.6
LR:  0.001
replay buffer size:  180646
Time Taken :  441.0  mins 44.43910789489746  seconds
[[[31704. 15915.  3543.   128.     0.     0.]
  [23700. 11860.  2708.   606.   144.     5.]
  [15366.  7802.  1996.   187.    57.    67.]
  [10538.  5416.  1077.   536.   459.   540.]
  [ 7843.  4372.  1447.   764.   101.   465.]
  [ 7494.  3599.   856.   177.    23.   255.]]]
step number:  34 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
step number:  19 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
step number:  26 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
step number:  18 total reward:  1.0  reward:  1.0 rdn_beta:  0.0
