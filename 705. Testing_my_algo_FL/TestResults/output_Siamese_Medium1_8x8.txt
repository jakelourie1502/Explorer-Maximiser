[['S' 'F' 'F' 'F' 'H' 'F' 'H' 'F']
 ['F' 'H' 'H' 'F' 'F' 'F' 'F' 'F']
 ['F' 'F' 'F' 'F' 'F' 'F' 'H' 'F']
 ['F' 'H' 'F' 'F' 'F' 'F' 'F' 'F']
 ['F' 'H' 'F' 'F' 'H' 'F' 'F' 'F']
 ['F' 'H' 'F' 'H' 'F' 'F' 'F' 'F']
 ['F' 'H' 'F' 'F' 'F' 'F' 'H' 'H']
 ['H' 'H' 'H' 'F' 'F' 'F' 'F' 'G']]
Adding thread: now have 13 threads
Starting evaluation
siam score:  -0.003313036
line 193 mcts: sample exp_bonus 0.0
deleting a thread, now have 12 threads
Frames:  1437 train batches done:  10 episodes:  146
Test reward:  0.0
Q value #end_state_>_threshold:  0
Q value #non_end_state_>threshold:  0
Scores:  {'0.0': 0}
Episodes: 156, frames: 1541, time: 71.31379985809326
training steps:  12
RDN obj mus: [0, 0, 0, 0, 0]
RDN obj sigmas: [1, 1, 1, 1, 1]
156 : 0.0
LR:  1.1e-05
replay buffer size:  4041
Time Taken :  1.0  mins 11.313636064529419  seconds
[[[380. 236.  86.  32.   7.   1.   1.   0.]
  [226. 110.  19.  15.   5.   7.   3.   1.]
  [ 94.  35.  12.   9.   8.   4.   2.   0.]
  [ 27.   7.  10.   7.   2.   4.   1.   0.]
  [  7.   6.   9.   2.   1.   1.   0.   0.]
  [  1.   3.   4.   0.   0.   0.   0.   0.]
  [  0.   0.   0.   0.   0.   0.   0.   0.]
  [  0.   0.   0.   0.   0.   0.   0.   0.]]]
deleting a thread, now have 11 threads
Frames:  1719 train batches done:  41 episodes:  171
deleting a thread, now have 10 threads
Frames:  1955 train batches done:  76 episodes:  194
deleting a thread, now have 9 threads
Frames:  2128 train batches done:  112 episodes:  215
siam score:  -0.64025986
siam score:  -0.6392898
siam score:  -0.62970346
siam score:  -0.60721666
siam score:  -0.5832148
deleting a thread, now have 8 threads
Frames:  2328 train batches done:  148 episodes:  234
siam score:  -0.5075999
siam score:  -0.43102813
siam score:  -0.33383614
siam score:  -0.5067923
siam score:  -0.4835213
siam score:  -0.50541186
siam score:  -0.5844166
siam score:  -0.5019543
siam score:  -0.4828449
siam score:  -0.73657304
siam score:  -0.7764271
siam score:  -0.78584075
siam score:  -0.78126425
siam score:  -0.765052
line 193 mcts: sample exp_bonus 0.0
line 193 mcts: sample exp_bonus 0.0
siam score:  -0.6173288
siam score:  -0.61321336
siam score:  -0.59088904
line 193 mcts: sample exp_bonus 0.0
siam score:  -0.73633987
siam score:  -0.73688835
siam score:  -0.7340628
siam score:  -0.6898908
siam score:  -0.66712683
UNIT TEST: sample policy line 217 mcts : [0.25  0.167 0.167 0.208 0.208]
Starting evaluation
Test reward:  0.0
Q value #end_state_>_threshold:  0
Q value #non_end_state_>threshold:  0
Scores:  {'0.0': 0}
Episodes: 919, frames: 10148, time: 2386.1886920928955
training steps:  1181
RDN obj mus: [0, 0, 0, 0, 0]
RDN obj sigmas: [1, 1, 1, 1, 1]
919 : 0.0
LR:  0.001
replay buffer size:  12934
Time Taken :  39.0  mins 46.188523054122925  seconds
[[[3079. 1584.  514.  193.   36.    1.    2.    3.]
  [1567.  645.  126.   55.   15.   12.    7.    3.]
  [ 594.  157.   55.   40.   19.    4.    4.    0.]
  [ 216.   67.   22.   18.   11.    5.    1.    0.]
  [  78.   20.    9.    3.    5.    1.    0.    0.]
  [  23.    7.    4.    1.    0.    0.    0.    0.]
  [  17.    4.    0.    0.    0.    0.    0.    0.]
  [   2.    0.    0.    0.    0.    0.    0.    0.]]]
siam score:  -0.5872573
UNIT TEST: sample policy line 217 mcts : [0.167 0.333 0.125 0.25  0.125]
siam score:  -0.57175356
siam score:  -0.6573085
siam score:  -0.6993835
line 193 mcts: sample exp_bonus 0.0
siam score:  -0.7311589
siam score:  -0.72561175
siam score:  -0.7456298
siam score:  -0.7478836
siam score:  -0.72544944
siam score:  -0.723204
siam score:  -0.7215322
UNIT TEST: sample policy line 217 mcts : [0.25  0.333 0.125 0.167 0.125]
siam score:  -0.5962904
line 193 mcts: sample exp_bonus 0.0
UNIT TEST: sample policy line 217 mcts : [0.167 0.125 0.208 0.25  0.25 ]
siam score:  -0.70649385
UNIT TEST: sample policy line 217 mcts : [0.25  0.125 0.125 0.167 0.333]
line 193 mcts: sample exp_bonus 0.0
line 193 mcts: sample exp_bonus 0.0
siam score:  -0.65919226
siam score:  -0.6490898
siam score:  -0.6545337
siam score:  -0.723872
line 193 mcts: sample exp_bonus 0.0
line 193 mcts: sample exp_bonus 0.0
line 193 mcts: sample exp_bonus 0.0
siam score:  -0.74293023
Starting evaluation
Test reward:  0.0
Q value #end_state_>_threshold:  0
Q value #non_end_state_>threshold:  0
Scores:  {'0.0': 0}
Episodes: 1790, frames: 20105, time: 5684.5769810676575
training steps:  2354
RDN obj mus: [0, 0, 0, 0, 0]
RDN obj sigmas: [1, 1, 1, 1, 1]
1790 : 0.0
LR:  0.001
replay buffer size:  23192
Time Taken :  94.0  mins 44.57681322097778  seconds
[[[6172. 3109. 1068.  353.   74.    1.    2.    3.]
  [3016. 1237.  246.  104.   37.   15.    7.    3.]
  [1196.  320.   93.   72.   38.    9.    7.    0.]
  [ 464.  142.   36.   36.   21.   11.    6.    1.]
  [ 192.   45.   20.   14.    8.    1.    0.    0.]
  [  69.   15.    6.    2.    0.    0.    0.    0.]
  [  31.    8.    1.    0.    0.    0.    0.    0.]
  [   4.    0.    0.    0.    0.    0.    0.    0.]]]
siam score:  -0.6423664
siam score:  -0.6388192
siam score:  -0.69212896
siam score:  -0.65796983
siam score:  -0.65498513
siam score:  -0.6677379
siam score:  -0.7150996
siam score:  -0.69345385
siam score:  -0.73814476
line 193 mcts: sample exp_bonus 0.0
siam score:  -0.7318674
siam score:  -0.7357606
line 193 mcts: sample exp_bonus 0.0
siam score:  -0.7359836
siam score:  -0.7518747
siam score:  -0.75672287
siam score:  -0.7252092
siam score:  -0.69170225
UNIT TEST: sample policy line 217 mcts : [0.167 0.292 0.292 0.125 0.125]
siam score:  -0.7028702
siam score:  -0.7387245
siam score:  -0.73591906
siam score:  -0.7327064
line 193 mcts: sample exp_bonus 0.0
siam score:  -0.732426
siam score:  -0.74206376
line 193 mcts: sample exp_bonus 0.0
siam score:  -0.71964365
siam score:  -0.706805
siam score:  -0.738485
Starting evaluation
Test reward:  0.0
Q value #end_state_>_threshold:  0
Q value #non_end_state_>threshold:  0
Scores:  {'0.0': 0}
Episodes: 2685, frames: 30126, time: 8997.094226837158
training steps:  3523
RDN obj mus: [0, 0, 0, 0, 0]
RDN obj sigmas: [1, 1, 1, 1, 1]
2685 : 0.0
LR:  0.001
replay buffer size:  33459
Time Taken :  149.0  mins 57.094064235687256  seconds
[[[9261. 4685. 1699.  577.  119.    7.    3.    3.]
  [4418. 1832.  380.  184.   55.   24.   10.    4.]
  [1737.  469.  162.  119.   57.   16.    8.    0.]
  [ 652.  224.   57.   57.   36.   16.    6.    1.]
  [ 260.   66.   21.   20.   13.    1.    0.    0.]
  [ 102.   22.    6.    4.    0.    0.    0.    0.]
  [  33.   10.    1.    0.    0.    0.    0.    0.]
  [   4.    0.    0.    0.    0.    0.    0.    0.]]]
siam score:  -0.6342811
siam score:  -0.63529795
siam score:  -0.6630536
siam score:  -0.7630825
siam score:  -0.76342666
line 193 mcts: sample exp_bonus 0.0
siam score:  -0.75967234
siam score:  -0.7548149
line 193 mcts: sample exp_bonus 0.0
siam score:  -0.76858443
siam score:  -0.767501
siam score:  -0.76517576
siam score:  -0.7539209
siam score:  -0.7673064
siam score:  -0.76372254
UNIT TEST: sample policy line 217 mcts : [0.25  0.125 0.208 0.25  0.167]
siam score:  -0.7222779
siam score:  -0.738743
siam score:  -0.7405063
siam score:  -0.73198235
siam score:  -0.74781466
siam score:  -0.7411682
siam score:  -0.7484166
line 193 mcts: sample exp_bonus 0.0
UNIT TEST: sample policy line 217 mcts : [0.25  0.125 0.25  0.167 0.208]
UNIT TEST: sample policy line 217 mcts : [0.458 0.167 0.125 0.125 0.125]
siam score:  -0.7215762
siam score:  -0.71757674
siam score:  -0.7291043
line 193 mcts: sample exp_bonus 0.0
UNIT TEST: sample policy line 217 mcts : [0.375 0.208 0.125 0.167 0.125]
siam score:  -0.69858843
Starting evaluation
Test reward:  0.0
Q value #end_state_>_threshold:  0
Q value #non_end_state_>threshold:  0
Scores:  {'0.0': 0}
Episodes: 3548, frames: 40148, time: 12327.121295928955
training steps:  4680
RDN obj mus: [0, 0, 0, 0, 0]
RDN obj sigmas: [1, 1, 1, 1, 1]
3548 : 0.0
LR:  0.001
replay buffer size:  43796
Time Taken :  205.0  mins 27.12113118171692  seconds
[[[12309.  6274.  2279.   786.   163.     7.     5.     3.]
  [ 5905.  2409.   517.   262.    83.    35.    15.     6.]
  [ 2329.   611.   225.   152.    67.    22.    11.     4.]
  [  865.   298.    74.    73.    41.    19.    12.    10.]
  [  335.    78.    27.    28.    14.     1.     6.     4.]
  [  128.    26.     9.     9.     0.     0.     3.     0.]
  [   42.    11.     1.     0.     0.     0.     1.     0.]
  [    6.     0.     0.     0.     0.     0.     0.     0.]]]
siam score:  -0.72960234
siam score:  -0.6741864
siam score:  -0.70168763
siam score:  -0.7287336
siam score:  -0.7303195
line 193 mcts: sample exp_bonus 0.0
siam score:  -0.754553
siam score:  -0.7448021
siam score:  -0.7516668
siam score:  -0.70321184
Showing what the boot values, the n step returns and the n step returns with V look like
tensor([[    0.0000],
        [    0.0000],
        [    0.0000],
        [    0.0000],
        [    0.0000],
        [    0.0000],
        [    0.0000],
        [    0.0000],
        [    0.0000],
        [    0.0000]], dtype=torch.float64)
0.0 2.054185982912323e-12
0.0 0.0
0.0 2.054185982912323e-12
0.0 2.054185982912323e-12
0.0 2.054185982912323e-12
0.0 2.054185982912323e-12
0.0 0.0
0.0 2.054185982912323e-12
0.0 2.054185982912323e-12
0.0 2.054185982912323e-12
siam score:  -0.7298867
siam score:  -0.746145
line 193 mcts: sample exp_bonus 0.0
siam score:  -0.7561098
Showing what the boot values, the n step returns and the n step returns with V look like
tensor([[    -0.0000],
        [    -0.0000],
        [    -0.0000],
        [    -0.0000],
        [    -0.0000],
        [    -0.0000],
        [    -0.0000],
        [    -0.0000],
        [    -0.0000],
        [    -0.0000]], dtype=torch.float64)
0.0 -3.805649804690188e-12
0.0 -2.5947611694158356e-13
0.0 -2.5947611694158356e-13
0.0 -3.805649804690188e-12
0.0 0.0
0.0 -3.805649804690188e-12
0.0 -3.805649804690188e-12
0.0 0.0
0.0 -3.805649804690188e-12
0.0 -3.805649804690188e-12
siam score:  -0.73620087
siam score:  -0.74363726
siam score:  -0.74536085
siam score:  -0.69476426
siam score:  -0.6838292
siam score:  -0.67851317
line 193 mcts: sample exp_bonus 0.0
Starting evaluation
Test reward:  0.0
Q value #end_state_>_threshold:  0
Q value #non_end_state_>threshold:  0
Scores:  {'0.0': 0}
Episodes: 4472, frames: 50073, time: 15856.534415960312
training steps:  5858
RDN obj mus: [0, 0, 0, 0, 0]
RDN obj sigmas: [1, 1, 1, 1, 1]
4472 : 0.0
LR:  0.001
replay buffer size:  54017
Time Taken :  264.0  mins 16.534247159957886  seconds
[[[15396.  7811.  2873.   970.   205.    11.     5.     3.]
  [ 7395.  3049.   658.   321.   106.    50.    16.     6.]
  [ 2848.   731.   285.   173.    90.    34.    12.     4.]
  [ 1048.   367.   102.    86.    52.    22.    12.    10.]
  [  398.   101.    31.    31.    16.     1.     6.     4.]
  [  144.    29.    11.    11.     0.     0.     3.     0.]
  [   44.    12.     1.     0.     0.     0.     1.     0.]
  [    6.     0.     0.     0.     0.     0.     0.     0.]]]
siam score:  -0.67001593
Showing what the boot values, the n step returns and the n step returns with V look like
tensor([[    -0.0000],
        [    -0.0000],
        [    -0.0000],
        [    -0.0000],
        [    -0.0000],
        [    -0.0000],
        [    -0.0000],
        [    -0.0000],
        [    -0.0000],
        [    -0.0000]], dtype=torch.float64)
0.0 -3.567796697963623e-12
0.0 -3.567796697963623e-12
0.0 0.0
0.0 0.0
0.0 -3.567796697963623e-12
0.0 0.0
0.0 0.0
0.0 -3.567796697963623e-12
0.0 0.0
0.0 -3.567796697963623e-12
siam score:  -0.70731956
siam score:  -0.66343385
UNIT TEST: sample policy line 217 mcts : [0.208 0.125 0.167 0.375 0.125]
UNIT TEST: sample policy line 217 mcts : [0.292 0.125 0.208 0.208 0.167]
siam score:  -0.7145279
siam score:  -0.7221315
siam score:  -0.7243533
siam score:  -0.69537187
siam score:  -0.6974618
siam score:  -0.61379355
UNIT TEST: sample policy line 217 mcts : [0.167 0.125 0.125 0.458 0.125]
UNIT TEST: sample policy line 217 mcts : [0.292 0.167 0.125 0.25  0.167]
siam score:  -0.56033987
siam score:  -0.72306967
siam score:  -0.7288292
siam score:  -0.7248344
siam score:  -0.72148824
siam score:  -0.71469915
siam score:  -0.68636614
siam score:  -0.67088926
siam score:  -0.67264646
siam score:  -0.71066964
siam score:  -0.71466714
siam score:  -0.71418244
UNIT TEST: sample policy line 217 mcts : [0.25  0.25  0.125 0.125 0.25 ]
Sims:  25 1 epoch:  59191 pick best:  False frame count:  59191
siam score:  -0.71300185
siam score:  -0.7132254
siam score:  -0.71398795
siam score:  -0.6963678
line 193 mcts: sample exp_bonus 0.0
siam score:  -0.6888646
Starting evaluation
Test reward:  0.0
Q value #end_state_>_threshold:  0
Q value #non_end_state_>threshold:  0
Scores:  {'0.0': 0}
Episodes: 5354, frames: 60182, time: 19191.651586055756
training steps:  7044
RDN obj mus: [0, 0, 0, 0, 0]
RDN obj sigmas: [1, 1, 1, 1, 1]
5354 : 0.0
LR:  0.001
replay buffer size:  64438
Time Taken :  319.0  mins 51.65142035484314  seconds
[[[18492.  9259.  3427.  1157.   251.    18.     6.     3.]
  [ 8927.  3623.   803.   392.   130.    58.    16.     6.]
  [ 3482.   889.   348.   219.   112.    42.    13.     4.]
  [ 1338.   450.   116.   100.    60.    23.    12.    10.]
  [  500.   122.    36.    37.    17.     1.     6.     4.]
  [  180.    34.    11.    14.     0.     0.     3.     0.]
  [   55.    14.     1.     0.     0.     0.     1.     0.]
  [    6.     0.     0.     0.     0.     0.     0.     0.]]]
siam score:  -0.6928879
line 193 mcts: sample exp_bonus 0.0
siam score:  -0.7073323
UNIT TEST: sample policy line 217 mcts : [0.333 0.167 0.167 0.208 0.125]
line 193 mcts: sample exp_bonus 0.0
line 193 mcts: sample exp_bonus 0.0
siam score:  -0.69952345
siam score:  -0.70151246
siam score:  -0.6850508
siam score:  -0.66694444
siam score:  -0.66605395
line 193 mcts: sample exp_bonus 0.0
siam score:  -0.6396027
siam score:  -0.6444128
siam score:  -0.63949937
line 193 mcts: sample exp_bonus 0.0
UNIT TEST: sample policy line 217 mcts : [0.167 0.125 0.125 0.25  0.333]
siam score:  -0.63353574
siam score:  -0.6984546
siam score:  -0.7030146
siam score:  -0.7037454
siam score:  -0.6889634
line 193 mcts: sample exp_bonus 0.0
Showing what the boot values, the n step returns and the n step returns with V look like
tensor([[    -0.0000],
        [    -0.0000],
        [    -0.0000],
        [    -0.0000],
        [    -0.0000],
        [    -0.0000],
        [    -0.0000],
        [    -0.0000],
        [    -0.0000],
        [    -0.0000]], dtype=torch.float64)
0.0 0.0
0.0 -1.2973806116928345e-12
0.0 -1.2973806116928345e-12
0.0 -1.2973806116928345e-12
0.0 0.0
0.0 -1.2973806116928345e-12
0.0 0.0
0.0 -1.2973806116928345e-12
0.0 0.0
0.0 0.0
line 193 mcts: sample exp_bonus 0.0
siam score:  -0.62663513
siam score:  -0.6967796
siam score:  -0.70091367
siam score:  -0.7142696
siam score:  -0.719035
siam score:  -0.636257
siam score:  -0.6992214
siam score:  -0.64393544
siam score:  -0.6432858
Starting evaluation
Test reward:  0.0
Q value #end_state_>_threshold:  0
Q value #non_end_state_>threshold:  0
Scores:  {'0.0': 0}
Episodes: 6255, frames: 70079, time: 22208.78751206398
training steps:  8207
RDN obj mus: [0, 0, 0, 0, 0]
RDN obj sigmas: [1, 1, 1, 1, 1]
6255 : 0.0
LR:  0.001
replay buffer size:  74593
Time Taken :  370.0  mins 8.78734016418457  seconds
[[[21513. 10814.  4074.  1410.   297.    18.     8.     3.]
  [10280.  4223.   947.   467.   151.    65.    19.     6.]
  [ 4047.  1020.   372.   253.   126.    47.    16.     4.]
  [ 1540.   532.   128.   111.    67.    26.    12.    10.]
  [  586.   136.    38.    39.    20.     1.     6.     4.]
  [  224.    35.    11.    14.     0.     0.     3.     0.]
  [   73.    16.     1.     0.     0.     0.     1.     0.]
  [   10.     0.     0.     0.     0.     0.     0.     0.]]]
